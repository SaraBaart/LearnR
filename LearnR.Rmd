---
title: "Getting started"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Once you have installed R and Rstudio, we can get started with R!
&nbsp;

**IMPORTANT** : R is case sensitive. This means that `mean(variable)` is not the same as `Mean(variable)`!
&nbsp;

## Overview 

The topics in this document are:

* [Packages](#packages) 

* [Importing data sets](#importing)

* [Datamanagement](#datamanagement)

* [Graphs](#graphs)

* [Descriptives](#descriptives)

* [Statistical tests](#tests)

* [Regression](#regression)

* [Save your work](#saving)

&nbsp;
&nbsp;

## Packages {#packages}


For a lot of R functions, additional libraries need to be loaded.

This can be done with:

```{r echo = T, eval=FALSE}
library(lattice)
```

Sometimes the library needs to be installed first:

```{r echo = T, eval=FALSE}
install.packages(lattice)
library(lattice)
```

*Installing* a library only needs to be done once. *Loading* the library must be done every time you start R. 

In the bottom right pane, you can find the tab *Packages*. Here you can find help on files about the packages. Help about a specific function can also be found with `?`:

```{r echo = T, eval=FALSE}
?xyplot
```



&nbsp;
&nbsp;

## Importing data sets {#importing}

How do I get my data in R?


#### SPSS file

1) Load the library foreign
```{r echo = T, eval=FALSE}
library(foreign)
```

2) Load data from spss using the function read.spss
```{r echo = T, eval=FALSE}
data <- read.spss("C:/Documents/data.sav", to.data.frame = TRUE)
```
&nbsp;

#### Excel file

1) Load the library readxl
```{r echo = T, eval=FALSE}
library(readxl)
```

2) Load data from spss using the function read_excel
```{r echo = T, eval=FALSE}
data <- read_excel("C:/Documents/data.xlsx")
```

&nbsp;

**Note 1** For R to recognize the folder you need to use forward slashes (`/`) instead of backward slashes (`\`)

**Note 2** Pay attention to the name you give your data set. You want it to be distinct, but don't make it too long!

&nbsp;

#### Importing data with the menu
Data can also be loaded via the menu. Go to File -> Import Dataset -> choose your filetype.

&nbsp;
&nbsp;

## Datamanagement {#datamanagement}
We  want to investigate and explore the data set before we perform our analyses. Often we also need to perform some data manipulation steps first.
&nbsp;

#### Explore data
Let's explore a dataset.

We will use a standard data set available in R (mtcars). I will name this dataset "data"
```{r echo = T, eval=T}
data(mtcars)
data <- mtcars
```

With the function `head`, we can print the first couple of rows; check out the names of the variables and the first entries.

```{r echo = T, eval=T}
head(data)
```
&nbsp;

##### Obtain the summaries of each variable
The numbers of missings `NA` are also reported.
```{r echo = T, eval=T}
summary(data)
```
&nbsp;

##### Obtain the summaries of one variable
With the `$` symbol we access a variable in a data set.
```{r echo = T, eval=T}
summary(data$mpg)
```
&nbsp;

#####  Obtain the variable names 
```{r echo = T, eval=T}
names(data)
```
&nbsp;

#####  Look at the type of variables in the data:

Numeric - Factor - character - Integer - etc
```{r echo = T, eval=T}
str(data)
```
&nbsp;

#### Basic data manipulation

##### Rename variables
```{r echo = T, eval=T}
names(data)[names(data) == "carb"] <- "CARB"
```
&nbsp;

##### Make a subset of your data
**Tip! ** Create a new data set with a new name. Otherwise you might lose important information in your data.

Remove observations with missings (`NA`) in "mpg" and "cyl"
```{r echo = T, eval=T}
data2 <- subset(data, !is.na(data$mpg) & !is.na(data$cyl))
```
&nbsp;

#####  Remove observation above a certain threshold
Make a new dataset, including only mpg values > 20. (Don't forget the comma in the code!)
```{r echo = T, eval=T}
data2 <- data[data$mpg > 20, ]
```
&nbsp;

##### Remove duplicates
This can be used if there are multiple measurement per patient, but you want to keep only the first.
```{r echo = T, eval=T}
data.short <- data[!duplicated(data$gear),]
```
&nbsp;

##### Remove variables
```{r echo = T, eval=T}
data2 <- subset(data, select = -c(mpg, cyl))
```
&nbsp;

#####  Keep variables
```{r echo = T, eval=T}
data2 <- subset(data, select = c(mpg, cyl))
```
&nbsp;
&nbsp;

## Graphs {#graphs}

Make a simple histogram (mpg = continuous variable)

```{r echo = T, eval=T}
hist(data$mpg)
```

Make a simple scatter plot (mpg & disp = continuous variables)
```{r echo = T, eval=T}
plot(data$mpg ~ data$disp)
```

Make a simple boxplot (mpg = continuous variable, cyl = categorical variable)
```{r echo = T, eval=T}
boxplot(data$mpg ~ data$cyl)
```


Improve the scatterplot by changing the color, adding a title, etc. 

```{r echo = T, eval=T}
plot(data$mpg ~ mtcars$disp, xlab = "Disp", ylab = "Mpg", 
     xlim = c(75, 475), ylim = c(10, 35),
     main = "Scatterplot Mpg - Disp", col = "red", pch = 20, cex = 1.5)
```

For two plots next to each other we can use the function `par(mfrow = c(1,2))`

```{r echo = T, eval=T}
par(mfrow = c(1,2))
plot(data$mpg ~ data$disp)
plot(data$mpg ~ mtcars$disp, xlab = "Disp", ylab = "Mpg", 
     xlim = c(75, 475), ylim = c(10, 35),
     main = "Scatterplot Mpg - Disp", col = "red", pch = 20, cex = 1.5)
```

&nbsp;

&nbsp;

## Descriptives {#descriptives}

##### Obtain descriptives for continuous variables

mean (sd)
```{r echo = T, eval=T}
mean(data$mpg)
sd(data$mpg)
```

median (IQR)
```{r echo = T, eval=T}
median(data$mpg)
quantile(data$mpg, probs = c(0.25, 0.75))
```
&nbsp;

##### Obtain descriptives for categorical variables

Frequencies
```{r echo = T, eval=T}
table(data$cyl)
```

Crosstabs
```{r echo = T, eval=T}
table(data$cyl, data$vs)
```

&nbsp;
&nbsp;

## Statistical tests {#tests}


#### T-test
With a t-test, we compare a normally distributed continuous variable between two groups. 
In the data "am" is a grouping variable (0/1). Let's explore this variable with `table`:
```{r echo = T, eval=T}
table(data$am)
```

"drat" is a continuous variable. Is it normally distributed?


```{r echo = T, eval=T}
par(mfrow = c(1,2))
hist(data$drat)
qqnorm(data$drat)
qqline(data$drat, col = "steelblue")
```

This looks pretty normal, so we can use a t-test to analyze these data. 

```{r echo = T, eval=T}
t.test(drat ~ am, data = data)
```

The output provides us with the p-value of the test, but also the 95\% confidence interval and the mean in both groups. 

It is possible to save the test as an r object (named "test") and extract the p-value from it.
```{r echo = T, eval=T}
test <- t.test(drat ~ am, data = data)
test$p.value
```
&nbsp;

#### Mann-Whitney U test
When the continuous variable is not normally distributed, we use the Mann-Whitney U test (or Wilcoxon Rank Sum test). The variable "mpg" is not normally distributed as we can see by the graphs.

```{r echo = T, eval=T}
par(mfrow = c(1,2))
hist(data$disp)
qqnorm(data$disp)
qqline(data$disp, col = "steelblue")
```

So, we use the code `wilcox.test`

```{r echo = T, eval=T, warning=FALSE}
wilcox.test(disp ~ am, data = data)
```
The p-value is <0.05, so we can reject H0.
&nbsp;

#### ANOVA
When we want to compare a continuous variable in more than 2 groups, we can use an ANOVA. For this test, we assume that the continuous marker follows a normal distribution. The variable "gear" has three groups:

```{r echo = T, eval=T}
table(data$gear)
```

We will use ANOVA to test if "drat" is similar over the "gear" groups. We visualize the data first:

```{r echo = T, eval=T}
boxplot(data$drat ~ data$gear)
```

There appears to be a big difference, especially in group 3. 
For the ANOVA procedure in R it is better if the grouping variable is a "factor" instead of "numeric", so we make a new variable: gear.factor.
```{r echo = T, eval=T}
str(data$gear)

data$gear.factor <- as.factor(data$gear)

str(data$gear.factor)
```


```{r echo = T, eval=T}
aov1 <- aov(drat ~ gear.factor, data = data)
summary(aov1)
```

The p-value is highly significant. We can perform a post-hoc test, to see which groups are different from each other. With Tukey we adjust the p-value to correct for multiple testing.

```{r echo = T, eval=T}
TukeyHSD(aov1)
```

Group 3 is indeed different from 4 and 5. 
&nbsp;

#### Kruskal-Wallis test
The non-paramtric version of the ANOVA is the Kruskal-Wallis test. 

```{r echo = T, eval=T}
boxplot(data$disp ~ data$gear.factor)
```

```{r echo = T, eval=T}
kruskal.test(disp ~ gear.factor, data = data)
```

The groups of gear are also significantly different for disp values.

&nbsp;


#### Chi-squared test
To see if two binary or categorical variables are associated, we can use the chi-squared test. 

The variables "vs" and "am" are both binary. With `table()` we can make a crosstab of the variables.

```{r echo = T, eval=T}
tab <- with(data, table(vs, am))
tab

```

We now estimate the chi-squared statistic on this table:

```{r echo = T, eval=T}
chisq.test(tab)
```
The p-value is > 0.05. 

&nbsp;

#### Correlations
Correlations are used for two continuous variables. We already made a scatterplot of "mpg" and "disp".

```{r echo = T, eval=T}
plot(data$mpg ~ data$disp)
```

There seems to be a strong negative correlation between the variables. Is it significant?

```{r echo = T, eval=T, warning=F}
cor.test(data$disp, data$mpg, method = "spearman")
```
Since "mpg" is not normally distributed, we use the `method = "spearman"` version of the test. For the Pearson correlation, use `method = "pearson"`.
The correlation is -0.9 and highly significant. 

&nbsp;

## Regression {#regression}
&nbsp;

####  Linear regression

In regression we like to explain how one variable (x) influences changes in a second variable (y). Let's start with a scatterplot for "drat" and "disp":

```{r echo = T, eval=T, warning=F}
plot(drat ~ disp, data = data)
```
&nbsp;

With the regression model we can estimate how unit-value in x (disp) impact a change in y (drat):
The model is estimated with `lm()`. `summary()` prints a summary of the model.

```{r echo = T, eval=T, warning=F}
m1 <- lm(drat ~ disp, data = data)
summary(m1)

```
&nbsp;

The p-value for disp is highly significant (`r summary(m1)$coefficients[2,4]`). I can extract the coefficient table from this object:
&nbsp;

```{r echo = T, eval=T, warning=F}
summary(m1)$coefficients
```
&nbsp;

To trust the results of the model, we need to validate the assumptions. Normality and homoskedasticity of the residuals can be checked by making plots.

```{r echo = T, eval=T, warning=F}
par(mfrow = c(2,2))
plot(m1)
```

We look mainly at the two upper plots. With the first we can check whether the residuals from a noisy cloud of points. The qq-plot in the second graph can show whether the residuals follow a normal distribution. No serious deviations here. 
&nbsp;


Multiple independent (x) variables can be added to the model.

```{r echo = T, eval=T, warning=F}
m2 <- lm(drat ~ disp + mpg + vs, data = data)
summary(m2)

```
&nbsp;

Interactions can be added by using the `*` instead of the `+`

```{r echo = T, eval=T, warning=F}
m3 <- lm(drat ~ disp * vs + mpg, data = data)
summary(m3)

```

Do not forget to evaluate the assumptions!

&nbsp;

#### Logistic regression
If the dependent variable is binary, we use a logistic regression model with `glm()`

```{r echo = T, eval=T, warning=F}
glm1 <- glm(vs ~ drat, data = data, family = binomial(logit))
summary(glm1)

```

&nbsp;

The value of the coefficient is interpreted by taking the exponent

```{r echo = T, eval=T, warning=F}
b1 <- summary(glm1)$coef[2, 1]
 exp(b1)

```

&nbsp;

Multiple variables can be added to the model

```{r echo = T, eval=T, warning=F}
glm2 <- glm(vs ~ drat + am + mpg, data = data, family = binomial(logit))
summary(glm2)

```

&nbsp;


The predicted probability of 'vs' for specific values of the x variables can be calculated with `predict()`

```{r echo = T, eval=T, warning=F}
DF <- data.frame(drat = 3.6, am = 1, mpg = 20.1)
predict(glm2, newdata = DF, type = "response")

```
The option `type = "response"` ensure the result is given als probability.


&nbsp;
&nbsp;

## Saving your work {#saving}
There are several ways to save your work. 
&nbsp;

#### Save script.
I always save my script as `.R` file. 
Additionally I can save separate R objects with the `save()` function. 

&nbsp; 

#### Save datasets and output
If `data2` is the result of several datamanagement steps and the dataset that is used for the analyses, I might want to save data. I can save this as `.RData` file.  

```{r echo = T, eval=F, warning=F}
save(data2, file = "C:/Documents/data2.RData")
```


**Reminder note ** for R to recognize the folder you need to use forward slashes (`/`) instead of backward slashes (`\`)

Models that take a long time to run, can also be save in this way.

```{r echo = T, eval=F, warning=F}
save(Model1, file = "C:/Documents/model1.RData")
```
&nbsp;

#### Export data 
If I want to use my data in excel, I can export the data to excel for example.

```{r echo = T, eval=F, warning=F}
library("xlsx")
save(data2, file = "C:/Documents/data2.xlsx")
```
&nbsp;

#### Save graphs

Graphs can be saved in different file formats use `png(), pdf(), tiff() etc.` to create the file. Then run the code for the plot. With `dev.off()` at the end, the plot is saved as the file. 
```{r echo = T, eval=F, warning=F}
png("C:/Documents/plots.png")
plot(data$mpg ~ mtcars$disp, xlab = "Disp", ylab = "Mpg")
dev.off()
```
&nbsp;

#### Save environment
You can also save the whole environment you are working in:
```{r echo = T, eval=F, warning=F}

save.image(file = "C:/Documents/MyEnvironment.RData")
```
